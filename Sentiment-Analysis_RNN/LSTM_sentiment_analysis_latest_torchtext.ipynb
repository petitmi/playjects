{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petitmi/Deep_learning-Sequential_data/blob/main/LSTM_sentiment_analysis_latest_torchtext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 586-lab4"
      ],
      "metadata": {
        "id": "KH80PJNJcUfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will use custom CSV text dataset for training a simple RNN for sentiment classification (here: a binary classification problem with two labels, positive and negative) using LSTM (Long Short Term Memory) cells and GRU Cells."
      ],
      "metadata": {
        "id": "ow5wuhTmeiTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/586'\n",
        "!ls \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdNkULzceZln",
        "outputId": "42412714-3327-4e55-bbe7-19ae9f0e7b52"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/586\n",
            "cnn-lenet5-cifar10.ipynb  data\tLab4.ipynb  notebookc0cd28f5bd.ipynb  Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --upgrade\n",
        "!pip install torchtext --upgrade"
      ],
      "metadata": {
        "id": "UO0h61fNythI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from nltk.corpus import stopwords \n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "HFK22pmTcUfr"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cells will download the IMDB movie review dataset (http://ai.stanford.edu/~amaas/data/sentiment/) for positive-negative sentiment classification in as CSV-formatted file:"
      ],
      "metadata": {
        "id": "12UCAcsMexDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !gunzip -f movie_data.csv.gz \n",
        "base_csv = 'data/movie_data.csv'\n",
        "df = pd.read_csv(base_csv)\n",
        "df.head()"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RYT9Cn_qcUfv",
        "outputId": "ac86edf6-1a76-46eb-f1ba-e94371b0c424"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5484440-37c5-48c4-ba23-7fc63c5030fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5484440-37c5-48c4-ba23-7fc63c5030fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5484440-37c5-48c4-ba23-7fc63c5030fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5484440-37c5-48c4-ba23-7fc63c5030fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting to train and test data"
      ],
      "metadata": {
        "id": "LOc5b5bBcUfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will split data to train and test initially."
      ],
      "metadata": {
        "id": "7P0zO7JAcUfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 123\n",
        "\n",
        "import random\n",
        "raw_data=df[['sentiment','review']].values.tolist()\n",
        "train_data, valid_data, test_data = random_split(raw_data, [0.75,0.05,0.2], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Valid: {len(valid_data)}')\n",
        "print(f'Num Test: {len(test_data)}')\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvV9DoDFcUfx",
        "outputId": "a46d6213-d301-4fac-c6d6-2e49cbb2ddef"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train: 37500\n",
            "Num Valid: 2500\n",
            "Num Test: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysing sentiment"
      ],
      "metadata": {
        "id": "nl-Ef21JcUfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tockenization\n",
        "\n",
        "**<font color='red'>Note</font>**: I will use the latest version(i.e. `0.15.0`) of `torchtext` instead of the example(i.e. `0.9.0`). \n",
        "- Use `data.utils.get_tokenizer` instead of `data.Field`\n",
        "- `vocab` is also different\n",
        "- Padding process are differene\n"
      ],
      "metadata": {
        "id": "CIIMlnEecUfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "metadata": {
        "trusted": true,
        "id": "PQZyiEzDcUf0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "counter = Counter()\n",
        "for line in df['review']:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = vocab(counter, min_freq=1, specials=('<unk>', '<BOS>', '<EOS>', '<PAD>'))"
      ],
      "metadata": {
        "id": "XdcrNUnTgSb_"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The length of the new vocab is\", len(vocab))\n",
        "new_stoi = vocab.get_stoi()\n",
        "print(\"The index of '<BOS>' is\", new_stoi['<BOS>'])\n",
        "new_itos = vocab.get_itos()\n",
        "print(\"The token at index 2 is\", new_itos[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7tITXHtiJyX",
        "outputId": "20ce5eda-1065-48e1-a148-79c16a40f780"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the new vocab is 147160\n",
            "The index of '<BOS>' is 1\n",
            "The token at index 2 is <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_transform = lambda x: [vocab['<BOS>']] + [vocab[token] for token in tokenizer(x)] + [vocab['<EOS>']]\n",
        "label_transform = lambda x: float(1) if x == 1 else 0\n",
        "\n",
        "print(\"input to the text_transform:\", 'this is me')\n",
        "print(\"output of the text_transform:\", text_transform( 'this is me'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bJvSZFIiGuH",
        "outputId": "1de59a7e-520d-4fea-9261-99aab17087e3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input to the text_transform: this is me\n",
            "output of the text_transform: [1, 181, 49, 272, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Length of vocabulary is {len(vocab)}')"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_H-gzvWcUf3",
        "outputId": "d1c04e62-e378-4c32-e383-13e77c336e03"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocabulary is 147160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Padding, batching and loading as tensor"
      ],
      "metadata": {
        "id": "z2HN2sOVcUf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# VOCABULARY_SIZE = 20000\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 15\n",
        "\n",
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n"
      ],
      "metadata": {
        "id": "r62dzUJNHmNJ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Using `DataLoader` to load the data\n",
        "- Using `collate_fn` function when loading to implement padding"
      ],
      "metadata": {
        "id": "8CIiNG0fh0hR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "\n",
        "def custom_collate(data): #(2)\n",
        "    inputs = [torch.tensor(text_transform(d[1])) for d in data] #(3)\n",
        "    labels = torch.tensor([label_transform(d[0]) for d in data])\n",
        "    inputs_lens = torch.tensor([len(x) for x in inputs])\n",
        "    inputs = pad_sequence(inputs, batch_first=True) #(4)\n",
        "    # inputs = [(d) for d in inputs]\n",
        "\n",
        "    return labels.to(DEVICE),inputs.to(DEVICE),inputs_lens.to(DEVICE)\n"
      ],
      "metadata": {
        "id": "wxTu60fcrL8N"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the outcome of this function."
      ],
      "metadata": {
        "id": "SzV1oNgBiP6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_collate(list(train_data)[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lub7LIVCPFdu",
        "outputId": "32916a59-cab9-4391-f527-fc24a7098c31"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 1., 0.], device='cuda:0'),\n",
              " tensor([[    1, 71932,    49,  ...,  3856,    24,     2],\n",
              "         [    1,  2382, 39360,  ...,     0,     0,     0],\n",
              "         [    1, 43646,  1841,  ...,     0,     0,     0]], device='cuda:0'),\n",
              " tensor([528,  54,  59], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE,collate_fn=custom_collate)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE,collate_fn=custom_collate)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE,collate_fn=custom_collate)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HRcU1UWVcUf6"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look at the data, get one:"
      ],
      "metadata": {
        "id": "B6mXXGqfiUvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teJYXQLGTpLM",
        "outputId": "fabb9935-66ea-4006-8fd0-a4494b87efff"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
              "         1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
              "         1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
              "         1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
              "         0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
              "         1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0., 1.], device='cuda:0'),\n",
              " tensor([[    1,   181,   106,  ...,     0,     0,     0],\n",
              "         [    1,   457,   153,  ...,     0,     0,     0],\n",
              "         [    1,   181,   637,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [    1,   258,  4717,  ...,     0,     0,     0],\n",
              "         [    1,    50,   371,  ...,     0,     0,     0],\n",
              "         [    1,  1055, 10006,  ...,     0,     0,     0]], device='cuda:0'),\n",
              " tensor([  46,  558,  196,  358,  197,  391,  164,  168,  150,  160,  572,  194,\n",
              "          175,  144,  181,  299,  254,  127,  523,   60,  149,  171, 1096,  138,\n",
              "          296,   75,  175,  113,  207,  163,  171,  169,  309,  166,  884,  684,\n",
              "          222,  143,  340,  234,  253,  421,  199,  149,  341,  183,  276,  112,\n",
              "          428, 1110,  260,  278,  293,   83,  180, 1060,  230,  322,  162,  182,\n",
              "          195,  337,  533,  177,  569,  733,  126,  138,  410,  298,  157,  240,\n",
              "          529,  412,  348,  470,  250,  331,  243,  327,   95,  154,  130,  371,\n",
              "          170,  195,  350,  145,  228,  537,  143,  196,  156,  114,  200,  442,\n",
              "          174,  147,   97,  378,  300,  240,  413,   99,  484,  272,  443,  231,\n",
              "          163, 1030,   84,  244,   66,  333,  164,   69,  150,  156,  307,  220,\n",
              "          122,  167,  137,  191,  234,  144,  268,  167], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "rUlZ-EGOcUf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        #Here is a preliminary model using LSTM cell\n",
        "        #The primary goal of this lab is to vary the dimensions of the embeddings and see the results\n",
        "        #The second task is to use a another RNN cell such as GRU and perform parameter tuning and report the results.\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text, text_length):\n",
        "        embedded = self.embedding(text)\n",
        "        packed = pack_padded_sequence(embedded, text_length.to('cpu'),batch_first=True,enforce_sorted=False) # Related to the previous pad_sequence\n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        # print(packed_output, (hidden, cell))\n",
        "        return self.fc(hidden.squeeze(0)).view(-1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ganMeTXCcUf7"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0wxDieHjcUf8"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "-ULTozuQcUf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_binary_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, text_length) in enumerate(data_loader):\n",
        "            logits = model(text,text_length)\n",
        "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
        "            num_examples += label.size(0)#change\n",
        "            correct_pred += (predicted_labels.long() == label.long()).sum()#change\n",
        "        return correct_pred.float()/num_examples * 100"
      ],
      "metadata": {
        "trusted": true,
        "id": "C4WtwKxTcUf8"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for idx, (label, text,text_length) in enumerate(train_loader):\n",
        "        \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(text, text_length)\n",
        "        cost = F.binary_cross_entropy_with_logits(logits, label)\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not idx % 50:\n",
        "             print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {idx:03d}/{len(train_loader):03d} | '\n",
        "                   f'Cost: {cost:.4f}')\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{compute_binary_accuracy(model, train_loader, DEVICE):.2f}%'\n",
        "              f'\\nvalid accuracy: '\n",
        "              f'{compute_binary_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}%')"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtFPuxqJcUf9",
        "outputId": "81b0778a-c702-4ae5-e8a3-21cc94d9cb5b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/015 | Batch 000/293 | Cost: 0.6918\n",
            "Epoch: 001/015 | Batch 050/293 | Cost: 0.6854\n",
            "Epoch: 001/015 | Batch 100/293 | Cost: 0.6944\n",
            "Epoch: 001/015 | Batch 150/293 | Cost: 0.6865\n",
            "Epoch: 001/015 | Batch 200/293 | Cost: 0.6266\n",
            "Epoch: 001/015 | Batch 250/293 | Cost: 0.5758\n",
            "training accuracy: 71.58%\n",
            "valid accuracy: 70.08%\n",
            "Time elapsed: 0.87 min\n",
            "Epoch: 002/015 | Batch 000/293 | Cost: 0.6088\n",
            "Epoch: 002/015 | Batch 050/293 | Cost: 0.4727\n",
            "Epoch: 002/015 | Batch 100/293 | Cost: 0.4751\n",
            "Epoch: 002/015 | Batch 150/293 | Cost: 0.4879\n",
            "Epoch: 002/015 | Batch 200/293 | Cost: 0.4724\n",
            "Epoch: 002/015 | Batch 250/293 | Cost: 0.4567\n",
            "training accuracy: 80.12%\n",
            "valid accuracy: 77.60%\n",
            "Time elapsed: 1.74 min\n",
            "Epoch: 003/015 | Batch 000/293 | Cost: 0.4758\n",
            "Epoch: 003/015 | Batch 050/293 | Cost: 0.4403\n",
            "Epoch: 003/015 | Batch 100/293 | Cost: 0.3637\n",
            "Epoch: 003/015 | Batch 150/293 | Cost: 0.5305\n",
            "Epoch: 003/015 | Batch 200/293 | Cost: 0.3574\n",
            "Epoch: 003/015 | Batch 250/293 | Cost: 0.3941\n",
            "training accuracy: 83.37%\n",
            "valid accuracy: 81.72%\n",
            "Time elapsed: 2.61 min\n",
            "Epoch: 004/015 | Batch 000/293 | Cost: 0.4196\n",
            "Epoch: 004/015 | Batch 050/293 | Cost: 0.4058\n",
            "Epoch: 004/015 | Batch 100/293 | Cost: 0.4298\n",
            "Epoch: 004/015 | Batch 150/293 | Cost: 0.3602\n",
            "Epoch: 004/015 | Batch 200/293 | Cost: 0.3240\n",
            "Epoch: 004/015 | Batch 250/293 | Cost: 0.3942\n",
            "training accuracy: 86.01%\n",
            "valid accuracy: 83.52%\n",
            "Time elapsed: 3.49 min\n",
            "Epoch: 005/015 | Batch 000/293 | Cost: 0.3270\n",
            "Epoch: 005/015 | Batch 050/293 | Cost: 0.3793\n",
            "Epoch: 005/015 | Batch 100/293 | Cost: 0.3639\n",
            "Epoch: 005/015 | Batch 150/293 | Cost: 0.3336\n",
            "Epoch: 005/015 | Batch 200/293 | Cost: 0.2922\n",
            "Epoch: 005/015 | Batch 250/293 | Cost: 0.2797\n",
            "training accuracy: 87.45%\n",
            "valid accuracy: 84.64%\n",
            "Time elapsed: 4.35 min\n",
            "Epoch: 006/015 | Batch 000/293 | Cost: 0.2786\n",
            "Epoch: 006/015 | Batch 050/293 | Cost: 0.3717\n",
            "Epoch: 006/015 | Batch 100/293 | Cost: 0.2744\n",
            "Epoch: 006/015 | Batch 150/293 | Cost: 0.2626\n",
            "Epoch: 006/015 | Batch 200/293 | Cost: 0.2988\n",
            "Epoch: 006/015 | Batch 250/293 | Cost: 0.2402\n",
            "training accuracy: 88.52%\n",
            "valid accuracy: 84.80%\n",
            "Time elapsed: 5.20 min\n",
            "Epoch: 007/015 | Batch 000/293 | Cost: 0.2831\n",
            "Epoch: 007/015 | Batch 050/293 | Cost: 0.3142\n",
            "Epoch: 007/015 | Batch 100/293 | Cost: 0.3275\n",
            "Epoch: 007/015 | Batch 150/293 | Cost: 0.3259\n",
            "Epoch: 007/015 | Batch 200/293 | Cost: 0.3746\n",
            "Epoch: 007/015 | Batch 250/293 | Cost: 0.2702\n",
            "training accuracy: 88.10%\n",
            "valid accuracy: 85.20%\n",
            "Time elapsed: 6.07 min\n",
            "Epoch: 008/015 | Batch 000/293 | Cost: 0.2872\n",
            "Epoch: 008/015 | Batch 050/293 | Cost: 0.3300\n",
            "Epoch: 008/015 | Batch 100/293 | Cost: 0.2753\n",
            "Epoch: 008/015 | Batch 150/293 | Cost: 0.2571\n",
            "Epoch: 008/015 | Batch 200/293 | Cost: 0.2505\n",
            "Epoch: 008/015 | Batch 250/293 | Cost: 0.2289\n",
            "training accuracy: 89.74%\n",
            "valid accuracy: 85.80%\n",
            "Time elapsed: 6.95 min\n",
            "Epoch: 009/015 | Batch 000/293 | Cost: 0.1891\n",
            "Epoch: 009/015 | Batch 050/293 | Cost: 0.2032\n",
            "Epoch: 009/015 | Batch 100/293 | Cost: 0.3297\n",
            "Epoch: 009/015 | Batch 150/293 | Cost: 0.3892\n",
            "Epoch: 009/015 | Batch 200/293 | Cost: 0.2368\n",
            "Epoch: 009/015 | Batch 250/293 | Cost: 0.2196\n",
            "training accuracy: 91.33%\n",
            "valid accuracy: 86.52%\n",
            "Time elapsed: 7.83 min\n",
            "Epoch: 010/015 | Batch 000/293 | Cost: 0.2767\n",
            "Epoch: 010/015 | Batch 050/293 | Cost: 0.2090\n",
            "Epoch: 010/015 | Batch 100/293 | Cost: 0.2566\n",
            "Epoch: 010/015 | Batch 150/293 | Cost: 0.2621\n",
            "Epoch: 010/015 | Batch 200/293 | Cost: 0.3508\n",
            "Epoch: 010/015 | Batch 250/293 | Cost: 0.1687\n",
            "training accuracy: 91.95%\n",
            "valid accuracy: 86.56%\n",
            "Time elapsed: 8.70 min\n",
            "Epoch: 011/015 | Batch 000/293 | Cost: 0.0933\n",
            "Epoch: 011/015 | Batch 050/293 | Cost: 0.2285\n",
            "Epoch: 011/015 | Batch 100/293 | Cost: 0.2524\n",
            "Epoch: 011/015 | Batch 150/293 | Cost: 0.1700\n",
            "Epoch: 011/015 | Batch 200/293 | Cost: 0.2303\n",
            "Epoch: 011/015 | Batch 250/293 | Cost: 0.2190\n",
            "training accuracy: 92.84%\n",
            "valid accuracy: 87.44%\n",
            "Time elapsed: 9.57 min\n",
            "Epoch: 012/015 | Batch 000/293 | Cost: 0.1903\n",
            "Epoch: 012/015 | Batch 050/293 | Cost: 0.2359\n",
            "Epoch: 012/015 | Batch 100/293 | Cost: 0.2009\n",
            "Epoch: 012/015 | Batch 150/293 | Cost: 0.2063\n",
            "Epoch: 012/015 | Batch 200/293 | Cost: 0.1945\n",
            "Epoch: 012/015 | Batch 250/293 | Cost: 0.1889\n",
            "training accuracy: 93.48%\n",
            "valid accuracy: 87.32%\n",
            "Time elapsed: 10.44 min\n",
            "Epoch: 013/015 | Batch 000/293 | Cost: 0.2157\n",
            "Epoch: 013/015 | Batch 050/293 | Cost: 0.1878\n",
            "Epoch: 013/015 | Batch 100/293 | Cost: 0.2237\n",
            "Epoch: 013/015 | Batch 150/293 | Cost: 0.2498\n",
            "Epoch: 013/015 | Batch 200/293 | Cost: 0.1793\n",
            "Epoch: 013/015 | Batch 250/293 | Cost: 0.2137\n",
            "training accuracy: 94.49%\n",
            "valid accuracy: 87.76%\n",
            "Time elapsed: 11.29 min\n",
            "Epoch: 014/015 | Batch 000/293 | Cost: 0.1415\n",
            "Epoch: 014/015 | Batch 050/293 | Cost: 0.1820\n",
            "Epoch: 014/015 | Batch 100/293 | Cost: 0.2354\n",
            "Epoch: 014/015 | Batch 150/293 | Cost: 0.2160\n",
            "Epoch: 014/015 | Batch 200/293 | Cost: 0.1482\n",
            "Epoch: 014/015 | Batch 250/293 | Cost: 0.1138\n",
            "training accuracy: 92.47%\n",
            "valid accuracy: 86.60%\n",
            "Time elapsed: 12.37 min\n",
            "Epoch: 015/015 | Batch 000/293 | Cost: 0.1822\n",
            "Epoch: 015/015 | Batch 050/293 | Cost: 0.1246\n",
            "Epoch: 015/015 | Batch 100/293 | Cost: 0.1785\n",
            "Epoch: 015/015 | Batch 150/293 | Cost: 0.1135\n",
            "Epoch: 015/015 | Batch 200/293 | Cost: 0.4070\n",
            "Epoch: 015/015 | Batch 250/293 | Cost: 0.0988\n",
            "training accuracy: 94.49%\n",
            "valid accuracy: 87.12%\n",
            "Time elapsed: 13.31 min\n",
            "Total Training Time: 13.31 min\n",
            "Test accuracy: 87.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inferance"
      ],
      "metadata": {
        "id": "PO7xzhrwcUgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    # based on:\n",
        "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
        "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
        "    model.eval()\n",
        "    # tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    input = torch.tensor(text_transform(sentence)).unsqueeze(dim=1).to(DEVICE)\n",
        "    length = torch.tensor([len(input)]).to(DEVICE)\n",
        "    input = pad_sequence(input)\n",
        "    prediction = torch.sigmoid(model(input, length))\n",
        "    \n",
        "    return prediction.item()"
      ],
      "metadata": {
        "id": "_5TVc9FgEJmN"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(model,\"I love it so much!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2Fi_2y-EgBU",
        "outputId": "c9d004a2-e017-4e5a-8fcd-0898b6ee331d"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8334588408470154"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(model,\"I hate it, so foolish and sucks!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGe5XOylEpZR",
        "outputId": "bd58402d-c49a-48e4-b011-c557aa6cdb0f"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25770819187164307"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    }
  ]
}