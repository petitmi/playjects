{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petitmi/FasterRCNN-ObjectDetection-VOC/blob/main/FasterRCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CINjibxJgNEL",
        "outputId": "0be66577-f64c-480f-dc90-774c81f073a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lWiBplSfhCqW"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('/content/drive/MyDrive/586/Project/data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTDS4NqEnRnY"
      },
      "source": [
        "## Data Processing\n",
        "The dataset is organized into several directories, each with a specific purpose.\n",
        "1. `Annotations`: This directory contains the ground truth annotations for each image in the dataset. The annotations are stored in XML files, with one file per image. These files provide information about the objects present in the image, such as the object class, bounding box coordinates (xmin, ymin, xmax, ymax), and other relevant attributes (e.g., whether the object is difficult or truncated).\n",
        "\n",
        "2. `ImageSets`: This directory contains text files that define different subsets of the dataset, such as training, validation, and testing splits. The files are organized into subdirectories according to the specific task (e.g., Main, Layout, Segmentation, etc.). Each file lists the image names (without the file extension) included in the particular subset. For example, train.txt lists the images used for training, and val.txt lists the images used for validation.\n",
        "\n",
        "3. `JPEGImages`: This directory contains the actual image files in JPEG format. The images are named using a unique identifier (e.g., 000001.jpg, 000002.jpg, etc.). These images are the input data for training and evaluation of object classification, detection, and segmentation models.\n",
        "\n",
        "4. `SegmentationClass` and `SegmentationObject`: These two directories contains segmentation masks for semantic and instance segmentation, which is not what this project would cover."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb-9Eu8u5xhJ",
        "outputId": "16f3e434-72fe-4fbe-8415-ffb9ec4db5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.11.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6HY8-YVnBRf"
      },
      "source": [
        "Install the necessary libraries.\n",
        "In addition, a list for converting indexes to object names and a dictionary for converting object names to indexes will be provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4svW2C7hNnc",
        "outputId": "5624d5e0-0543-4988-e4f6-68880b68c14e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'ambigious']\n",
            "{'__background__': 0, 'aeroplane': 1, 'bicycle': 2, 'bird': 3, 'boat': 4, 'bottle': 5, 'bus': 6, 'car': 7, 'cat': 8, 'chair': 9, 'cow': 10, 'diningtable': 11, 'dog': 12, 'horse': 13, 'motorbike': 14, 'person': 15, 'pottedplant': 16, 'sheep': 17, 'sofa': 18, 'train': 19, 'tvmonitor': 20, 'ambigious': 21}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert indexes to object names\n",
        "\n",
        "index2name = ['__background__',\n",
        "        'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
        "        'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
        "        'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n",
        "        'tvmonitor', 'ambigious'\n",
        "    ]\n",
        "\n",
        "print(index2name)\n",
        "\n",
        "name2index = dict(zip(index2name, range(len(index2name)+1)))\n",
        "print(name2index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_3fEiaPnJi_"
      },
      "source": [
        "### Function to process the xml data\n",
        "Bounding box and object name data are stored in xml format.\n",
        "Functions are provided to extract and prepare the necessary data from this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EsPDXn1ObiRY"
      },
      "outputs": [],
      "source": [
        "def arrange_target(target):\n",
        "    objects = target[\"annotation\"][\"object\"]\n",
        "    box_dics = [obj[\"bndbox\"] for obj in objects]\n",
        "    box_keys = [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
        "\n",
        "    # bounding box\n",
        "    boxes = []\n",
        "    for box_dic in box_dics:\n",
        "        box = [int(box_dic[key]) for key in box_keys]\n",
        "        boxes.append(box)\n",
        "    boxes = torch.tensor(boxes)\n",
        "\n",
        "    # Object Name\n",
        "    labels = [name2index[obj[\"name\"]] for obj in objects]  # Object names are converted to indexes\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    dic = {\"boxes\":boxes, \"labels\":labels}\n",
        "    return dic\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntm-GXoOpD0u"
      },
      "source": [
        "### Loading the data\n",
        "Load the \"Pascal VOC Detection Dataset\", a dataset provided by Torchvision.\n",
        "https://pytorch.org/vision/0.8/datasets.html#torchvision.datasets.VOCDetection\n",
        "Set the transform and target_transform settings to prepare the data for use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "akz8I5WKe94c"
      },
      "outputs": [],
      "source": [
        "\n",
        "ROOT = \"/content/drive/MyDrive/586/Project/data\"\n",
        "\n",
        "# if 'dataset_train' not in globals():\n",
        "dataset_train=torchvision.datasets.VOCDetection(root=ROOT,\n",
        "                                                year=\"2012\",\n",
        "                                                image_set=\"train\",\n",
        "                                                # download=True,\n",
        "                                                # download=False,\n",
        "                                                transform=transforms.ToTensor(),\n",
        "                                                target_transform=transforms.Lambda(arrange_target)\n",
        "                                                )\n",
        "# if 'dataset_test' not in globals():\n",
        "dataset_test=torchvision.datasets.VOCDetection(root=ROOT,\n",
        "                                                year=\"2012\",\n",
        "                                                image_set=\"val\",\n",
        "                                                # download=True,\n",
        "                                                # download=False,\n",
        "                                                transform=transforms.ToTensor(),\n",
        "                                                target_transform=transforms.Lambda(arrange_target)\n",
        "                                                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqYxlg3dnW8i"
      },
      "source": [
        "### Setting up a DataLoader\n",
        "Set up a DataLoader so that data can be retrieved little by little.  \n",
        "\n",
        "If `batch_size`>0,we need `torchvision.ops.collate_batch`, which takes a list of samples, where each sample is a tuple containing an image and its target (a dictionary with annotations). collate_batch correctly handles variable-length lists of target annotations and returns two lists: one for the batched images and another for the batched targets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iVZ31dWZ4Fe9"
      },
      "outputs": [],
      "source": [
        "if 'data_loader_train' in globals():\n",
        "    del data_loader_train\n",
        "if 'data_loader_test' in globals():\n",
        "    del data_loader_test\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=5\n",
        "NUM_WORKERS = 2  \n",
        "LEARNING_RATE = 1e-2\n",
        "EPOCH = 2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4\n",
        "STEP_SIZE=1\n",
        "GAMMA=0.1"
      ],
      "metadata": {
        "id": "xBsi60Bo7E26"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Tsl_ocl5iCWr"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "data_loader_train = DataLoader(dataset_train, \n",
        "                               batch_size=BATCH_SIZE, \n",
        "                               num_workers=NUM_WORKERS,\n",
        "                               shuffle=True, \n",
        "                               collate_fn=collate_fn)\n",
        "# \n",
        "data_loader_test = DataLoader(dataset_test, \n",
        "                              batch_size=BATCH_SIZE, \n",
        "                              num_workers=NUM_WORKERS,\n",
        "                              shuffle=True, \n",
        "                              collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9XnUhsEbxKz",
        "outputId": "181b6a26-5061-4c95-ee25-04cab9ebcdbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train: 5717\n",
            "Num Test: 5823\n"
          ]
        }
      ],
      "source": [
        "print(f\"Num Train: {len(data_loader_train.dataset)}\")\n",
        "print(f\"Num Test: {len(data_loader_test.dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLmX9iR4ncYN"
      },
      "source": [
        "### Display Targets\n",
        "Draws bounding boxes and labels on the image.\n",
        "The function `draw_bounding_boxes` takes three arguments - the input image, the bounding boxes, and the class labels (names) of the detected objects. The function is responsible for drawing the bounding boxes around the detected objects in the image and annotating them with the corresponding class labels.\n",
        "https://pytorch.org/vision/master/utils.html#torchvision.utils.draw_bounding_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XbBoXKprlrZ2"
      },
      "outputs": [],
      "source": [
        "# dataiter = iter(data_loader_train)  # iterator\n",
        "# image,target = next(dataiter)  # retrieve a batch: image, target\n",
        "# print(image[0].shape,'\\n',target[0]) # image shape: C,H,W\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8kSGCpif75u-"
      },
      "outputs": [],
      "source": [
        "# def show_boxes(image, boxes, names):\n",
        "#     drawn_boxes = draw_bounding_boxes(image, boxes, labels=names) \n",
        "#     plt.figure(figsize = (16,16))\n",
        "#     plt.imshow(np.transpose(drawn_boxes, (1, 2, 0)))  # change the order of the axes to (H, W, C) - imshow required.\n",
        "#     plt.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)  # Hide labels and memory\n",
        "#     plt.show()\n",
        "\n",
        "# image = image[0]\n",
        "# image = (image*255).to(torch.uint8)  # multiplying 255 means scaling the normalized values back to the range [0, 255]. \n",
        "\n",
        "# boxes = target[0][\"boxes\"]\n",
        "# labels = target[0][\"labels\"]\n",
        "# names = [index2name[label.item()] for label in labels] # change label index to words\n",
        "\n",
        "# show_boxes(image, boxes, names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55azQl4nnh68"
      },
      "source": [
        "## Building the model\n",
        "Set up a Faster R-CNN model with ResNet-50-FPN as the backbone and load the learned parameters.\n",
        "https://pytorch.org/vision/stable/models.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n",
        "\n",
        "Faster R-CNN consists of two main components: the Region Proposal Network (RPN) and the Fast R-CNN. The RPN is responsible for generating region proposals that potentially contain objects. Some of these proposals may cover only the background or irrelevant parts of the image. To handle this, the background class is added to the set of object classes during the training process.\n",
        "\n",
        "In our case, there are 20 types of objects, but since we want to include the background, we will add 1 to this number to create a 21-class classification. The background class represents regions in the image that do not contain any objects of interest, which is used to differentiate between the actual objects and the background.\n",
        "\n",
        "The box_predictor is generated using the `FastRCNNPredictor` class.\n",
        "https://github.com/pytorch/vision/blob/6d9a42c322cb815516d5ea556b751d0c7e767c7f/torchvision/models/detection/faster_rcnn.py#L285.\n",
        "\n",
        "`FastRCNNPredictor` is a component of the Faster R-CNN model, specifically the box predictor, which is responsible for predicting the class labels and bounding box refinements for the Region of Interest (ROI) proposals. The input data for FastRCNNPredictor is the pooled features from each region proposal, which are processed by the ROI pooling layer.\n",
        "\n",
        "The original paper on Faster R-CNN is here.\n",
        "https://arxiv.org/abs/1506.01497\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lRfOigyluX-",
        "outputId": "aaa5b1b5-878f-49fc-f7f8-081a9133d68a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:00<00:00, 226MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=23, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=92, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "\n",
        "NUM_CLASSES=len(index2name)  # Add 1 to classify including background class\n",
        "IN_FEATURES = model.roi_heads.box_predictor.cls_score.in_features # the number of input features for the classification head of the pre-trained Faster R-CNN model. \n",
        "\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(IN_FEATURES, NUM_CLASSES)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIdj6Mipnpdm"
      },
      "source": [
        "## Training\n",
        "Training is performed using the prepared data and the constructed model.\n",
        "The parameters are adjusted using back-propagation to reduce this error.\n",
        "1. `optimizer` uses Stochastic Gradient Descent (SGD) optimizer.\n",
        " - MOMENTUM helps the optimizer accelerate the convergence and avoid local minima.\n",
        " - The WEIGHT_DECAY (L2 regularization) helps prevent overfitting by penalizing large weights.\n",
        "\n",
        "2. `lr_scheduler` adjusts the learning rate during training. The `StepLR` scheduler updates the learning rate periodically based on the number of epochs, following a step-wise decay schedule. \n",
        " - STEP_SIZE is the number of epochs between learning rate adjustments.\n",
        " - GAMMA is the multiplicative factor (0, 1) by which the learning rate will be reduced at each step. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0GlMDthzFT"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "### Metrics\n",
        "https://torchmetrics.readthedocs.io/en/stable/detection/mean_average_precision.html\n",
        "![ev](https://manalelaidouni.github.io/assets/img/pexels/Detections_IoU_edit1-401ab064-29c4-4e74-8b34-b79b7b5cb6eb.png)\n",
        "\n",
        "- **[Primary] Mean Average Precision (mAP)**: mAP is the mean of the average precision scores for all object classes in the dataset. AP is  the area under the precision-recall curve for a particular class. \n",
        "- **Precision (P)**: ratio of true positives and all positive predictions. P =  TP / (TP + FP)\n",
        "- **Recall (R)**: ratio of true positives and all positive ground truth. R = TP / (TP + FN)\n",
        "- **F1 Score**: Harmonic mean of precision and recall. F1 = 2(P·R)/(P + R).\n",
        "- **Intersection over Union (IoU)**, a measure of the overlap between two bounding boxes (the predicted box and the ground truth box), determines if a prediction is a true positive or a false positive. A predicted bounding box is considered a TP if it has an IoU value above a certain threshold with a ground truth bounding box and has the correct class label. The threshold is usually 0.5.    \n",
        "\n",
        "\n",
        "### Challenge\n",
        "It's hard to draw precision-recall curve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict The Test Aata"
      ],
      "metadata": {
        "id": "ODZFizlIxAwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FJMoGCMNj4LD"
      },
      "outputs": [],
      "source": [
        "def get_evadata(data_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(image.to(DEVICE) for image in images)\n",
        "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            predictions = model(images)\n",
        "\n",
        "            for pred, target in zip(predictions, targets):\n",
        "                all_preds.append(pred)\n",
        "                all_targets.append(target)\n",
        "    return all_preds,all_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformation Function for Evalulation\n",
        "The precision, recall and f1 are calculated when score>0.5, since we plan to use the box and class results that have scores above 0.5."
      ],
      "metadata": {
        "id": "LDW6OD34wfT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCORE_THRESHOLD = 0.5\n",
        "IOU_THRESHOULD = 0.5"
      ],
      "metadata": {
        "id": "6mMk5HwZxQnM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_eva(pred, target):\n",
        "    boxes,labels,scores = [],[],[]\n",
        "\n",
        "    for i, score in enumerate(pred['scores']):\n",
        "        if score > SCORE_THRESHOLD:  # Extract scores greater than 0.5\n",
        "            boxes.append(pred['boxes'][i].cpu().tolist())\n",
        "            labels.append(pred['labels'][i].cpu().tolist())\n",
        "            scores.append(pred['scores'][i].cpu().tolist())\n",
        "    \n",
        "    if len(boxes)==0:\n",
        "        boxes.append([0,0,0,0])\n",
        "    if len(labels)==0:\n",
        "        labels.append(0)\n",
        "    if len(scores)==0:\n",
        "        scores.append(0)\n",
        "                \n",
        "    pred_boxes = np.array(boxes)\n",
        "    pred_labels = np.array(labels)\n",
        "    pred_scores = np.array(scores)\n",
        "\n",
        "    gt_boxes = target['boxes'].cpu().numpy()\n",
        "    gt_labels = target['labels'].cpu().numpy()\n",
        "\n",
        "    return (pred_boxes, pred_labels, pred_scores),(gt_boxes,gt_labels)"
      ],
      "metadata": {
        "id": "tF622PSJvVFJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate TP, FP, FN\n",
        "1. Obtain the ground-truth bounding boxes and their corresponding class labels.\n",
        "2. Obtain the predicted bounding boxes, their class labels, and their confidence scores from the object detection model.\n",
        "3. For each predicted bounding box, find the ground-truth bounding box with the highest IoU, considering only those with the same class label.\n",
        "4. If the highest IoU is greater than or equal to the threshold, mark the pair as a true positive (TP). If not, mark the prediction as a false positive (FP).\n",
        "Keep track of the matched ground-truth bounding boxes.\n",
        "5. At the end of the process, count the number of ground-truth bounding boxes that were not matched. These are the false negatives (FN)."
      ],
      "metadata": {
        "id": "q-ecrbQwwQzS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5LrMJyqL1tgE"
      },
      "outputs": [],
      "source": [
        "from torchvision.ops import box_iou\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, all_preds,all_targets):\n",
        "    # Initialize variables for computing evaluation metrics\n",
        "    tp, fp, fn = 0, 0, 0\n",
        "    \n",
        "    eva_preds=[]\n",
        "    for pred, target in zip(all_preds, all_targets):\n",
        "        (pred_boxes, pred_labels, pred_scores),(gt_boxes,gt_labels)=transform_eva(pred, target)\n",
        "        # eva_preds is for the mAP calculation\n",
        "        eva_pred=dict()\n",
        "        eva_pred['boxes']=torch.tensor(pred_boxes).to(DEVICE)\n",
        "        eva_pred['labels']=torch.tensor(pred_labels).to(DEVICE)\n",
        "        eva_pred['scores']=torch.tensor(pred_scores).to(DEVICE)\n",
        "        eva_preds.append(eva_pred)\n",
        "\n",
        "        iou_matrix = box_iou(torch.tensor(pred_boxes), torch.tensor(gt_boxes))# return Tensor[Num_Row of Box1, Num_Row of Box2]\n",
        "        iou_matrix = iou_matrix.cpu().numpy()\n",
        "\n",
        "        for j, pred_box in enumerate(pred_boxes):\n",
        "            max_iou = np.max(iou_matrix[j],0) # find the matching box with the largest IoU value of the original box\n",
        "            max_index = np.argmax(iou_matrix[j]) # corresponding index\n",
        "            if max_iou >= IOU_THRESHOULD and pred_labels[j] == gt_labels[max_index]: #IoU > threshold and class is correct\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        for k, gt_label in enumerate(gt_labels):\n",
        "            if gt_label not in pred_labels:\n",
        "                fn += 1\n",
        "\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return precision, recall, f1, eva_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put Together"
      ],
      "metadata": {
        "id": "upL9S-ZGJa2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kffitDislwg1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "2be735db-1b9c-4dd0-b9de-17930698e3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:, 01/02 | Batch: 000/1144 | Loss:, 3.5960\n",
            "Epoch:, 01/02 | Batch: 100/1144 | Loss:, 0.6392\n",
            "Epoch:, 01/02 | Batch: 200/1144 | Loss:, 0.5951\n",
            "Epoch:, 01/02 | Batch: 300/1144 | Loss:, 0.5503\n",
            "Epoch:, 01/02 | Batch: 400/1144 | Loss:, 0.3399\n",
            "Epoch:, 01/02 | Batch: 500/1144 | Loss:, 0.3968\n",
            "Epoch:, 01/02 | Batch: 600/1144 | Loss:, 0.4587\n",
            "Epoch:, 01/02 | Batch: 700/1144 | Loss:, 0.2419\n",
            "Epoch:, 01/02 | Batch: 800/1144 | Loss:, 0.2337\n",
            "Epoch:, 01/02 | Batch: 900/1144 | Loss:, 0.2291\n",
            "Epoch:, 01/02 | Batch: 1000/1144 | Loss:, 0.1327\n",
            "Epoch:, 01/02 | Batch: 1100/1144 | Loss:, 0.4755\n",
            "Time elapsed: 34.10 min\n",
            "Epoch:, 02/02 | Batch: 000/1144 | Loss:, 0.3511\n",
            "Epoch:, 02/02 | Batch: 100/1144 | Loss:, 0.2181\n",
            "Epoch:, 02/02 | Batch: 200/1144 | Loss:, 0.1284\n",
            "Epoch:, 02/02 | Batch: 300/1144 | Loss:, 0.2003\n",
            "Epoch:, 02/02 | Batch: 400/1144 | Loss:, 0.1800\n",
            "Epoch:, 02/02 | Batch: 500/1144 | Loss:, 0.2947\n",
            "Epoch:, 02/02 | Batch: 600/1144 | Loss:, 0.2711\n",
            "Epoch:, 02/02 | Batch: 700/1144 | Loss:, 0.1998\n",
            "Epoch:, 02/02 | Batch: 800/1144 | Loss:, 0.2952\n",
            "Epoch:, 02/02 | Batch: 900/1144 | Loss:, 0.3356\n",
            "Epoch:, 02/02 | Batch: 1000/1144 | Loss:, 0.2718\n",
            "Epoch:, 02/02 | Batch: 1100/1144 | Loss:, 0.2410\n",
            "Time elapsed: 64.81 min\n",
            "Total Training Time: 64.81 min\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "# The list comprehension iterates over all the parameters in the model \n",
        "# and selects only those that have requires_grad set to True. \n",
        "# These are the parameters that will be updated during the training process.\n",
        "optimizer = torch.optim.SGD(params, \n",
        "                            lr=LEARNING_RATE, \n",
        "                            momentum=MOMENTUM, \n",
        "                            weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
        "                                               step_size=STEP_SIZE, \n",
        "                                               gamma=GAMMA) \n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "losses = []\n",
        "NUM_TRAIN = len(data_loader_train)\n",
        "for epoch in range(EPOCH):\n",
        "    model.train()  # train model\n",
        "    for i, (image, target) in enumerate(data_loader_train):\n",
        "        image = list(image.to(DEVICE) for image in image) # The model can process both a single image and a list (or batch) of images. \n",
        "        target = [{k: v.to(DEVICE) for k, v in t.items()} for t in target] # Target is a list with dictionary as an element\n",
        "\n",
        "        loss_dic = model(image, target) # The model returned scores, bbox_deltas\n",
        "        loss = sum(loss for loss in loss_dic.values())  # Calculate the sum of errors\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i%100 == 0:  # Progress is displayed every 100 times\n",
        "            print(f\"Epoch:, {epoch+1:02d}/{EPOCH:02d}\",  \n",
        "                  f\"| Batch: {i:03d}/{NUM_TRAIN:03d}\",  \n",
        "                  f\"| Loss:, {loss.item():.4f}\") \n",
        "            losses.append(loss.item())\n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    # Update the learning rate\n",
        "    lr_scheduler.step()\n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.ylabel('Losses')\n",
        "plt.xlabel('Iteration')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds_train,all_targets_train = get_evadata(data_loader_train)\n",
        "precision_train, recall_train, f1_train, eva_preds_train = evaluate_model(model, all_preds_train,all_targets_train)\n",
        "print(f'TRAIN\\nPrecision: {precision_train:0.4f}\\nRecall: {recall_train:0.4f}\\nF1 Score: {f1_train:0.4f}')\n",
        "metric_train = MeanAveragePrecision()\n",
        "metric_train.update(eva_preds_train, all_targets_train)\n",
        "from pprint import pprint\n",
        "pprint(metric_train.compute())\n",
        "\n",
        "all_preds_test,all_targets_test = get_evadata(data_loader_test)\n",
        "precision_test, recall_test, f1_test, eva_preds_test = evaluate_model(model, all_preds_test, all_targets_test)\n",
        "print(f'\\nTEST\\nPrecision: {precision_test:0.4f}\\nRecall: {recall_test:0.4f}\\nF1 Score: {f1_test:0.4f}')\n",
        "metric_test = MeanAveragePrecision()\n",
        "metric_test.update(eva_preds_test, all_targets_test)\n",
        "pprint(metric_test.compute())"
      ],
      "metadata": {
        "id": "Y8S89J_7hO5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f284ddf-f372-4e49-e622-5a50f4901252"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN\n",
            "Precision: 0.6640\n",
            "Recall: 0.9587\n",
            "F1 Score: 0.7846\n",
            "{'map': tensor(0.5301),\n",
            " 'map_50': tensor(0.7961),\n",
            " 'map_75': tensor(0.6116),\n",
            " 'map_large': tensor(0.5628),\n",
            " 'map_medium': tensor(0.4901),\n",
            " 'map_per_class': tensor(-1.),\n",
            " 'map_small': tensor(0.2946),\n",
            " 'mar_1': tensor(0.4257),\n",
            " 'mar_10': tensor(0.6139),\n",
            " 'mar_100': tensor(0.6196),\n",
            " 'mar_100_per_class': tensor(-1.),\n",
            " 'mar_large': tensor(0.6550),\n",
            " 'mar_medium': tensor(0.5781),\n",
            " 'mar_small': tensor(0.3675)}\n",
            "\n",
            "TEST\n",
            "Precision: 0.6168\n",
            "Recall: 0.9242\n",
            "F1 Score: 0.7398\n",
            "{'map': tensor(0.4344),\n",
            " 'map_50': tensor(0.6860),\n",
            " 'map_75': tensor(0.4794),\n",
            " 'map_large': tensor(0.4880),\n",
            " 'map_medium': tensor(0.3357),\n",
            " 'map_per_class': tensor(-1.),\n",
            " 'map_small': tensor(0.1534),\n",
            " 'mar_1': tensor(0.3757),\n",
            " 'mar_10': tensor(0.5296),\n",
            " 'mar_100': tensor(0.5331),\n",
            " 'mar_100_per_class': tensor(-1.),\n",
            " 'mar_large': tensor(0.5892),\n",
            " 'mar_medium': tensor(0.4428),\n",
            " 'mar_small': tensor(0.2234)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyL1gR1Cnv1t"
      },
      "source": [
        "## Using Trained Models\n",
        "### Predict on one image\n",
        "\n",
        "The trained model is used to make predictions.\n",
        "At this point we do not consider the target score, so you will see a large number of bounding boxes and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNTJQyH9l2P_"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataiter = iter(data_loader_test)  # iterator\n",
        "image, target = next(dataiter)  # retrieve a batch\n",
        "\n",
        "image = list(image.to(DEVICE) for image in image)\n",
        "\n",
        "model.eval()\n",
        "predictions = model(image)\n",
        "\n",
        "image = (image[0]*255).to(torch.uint8).cpu() \n",
        "# print(predictions)\n",
        "\n",
        "# image = (image[0]*255).to(torch.uint8).cpu() \n",
        "# boxes = predictions[0][\"boxes\"].cpu()\n",
        "# labels = predictions[0][\"labels\"].cpu().detach().numpy()\n",
        "# labels = np.where(labels>=len(index2name), 0, labels)  # 0 if the label is out of range.\n",
        "# names = [index2name[label.item()] for label in labels]\n",
        "\n",
        "# print(names)\n",
        "# show_boxes(image, boxes, names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxlMuL1qn0VR"
      },
      "source": [
        "### Prediction result\n",
        "Filter the boxes with scores above 0.5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuPIdgKzl4qS"
      },
      "outputs": [],
      "source": [
        "\n",
        "boxes = []\n",
        "names = []\n",
        "\n",
        "# print(predictions)\n",
        "# print(list(target))\n",
        "\n",
        "(pred_boxes, pred_labels, pred_scores),(gt_boxes,gt_labels)=transform_eva(predictions[0],list(target)[0])\n",
        "\n",
        "if max(pred_scores)<=0.5:\n",
        "    max_idx = np.argmax(np.array(pred_scores))\n",
        "\n",
        "    box = pred_boxes[max_idx]\n",
        "    boxes.append(list(box))\n",
        "    label = pred_labels[max_idx]\n",
        "    names.append(index2name[label])\n",
        "else:\n",
        "    for i, box in enumerate(pred_boxes):\n",
        "        score = pred_scores[i]\n",
        "        box = pred_boxes[i]\n",
        "        label = pred_labels[i]\n",
        "        if score > 0.5:  # Extract scores greater than 0.5\n",
        "            if label >= len(index2name):  # 0 if the label is out of range.\n",
        "                label = 0\n",
        "            name = index2name[label]\n",
        "            names.append(name)\n",
        "            boxes.append(box)\n",
        "\n",
        "boxes = torch.tensor(boxes)\n",
        "\n",
        "show_boxes(image, boxes, names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coresponding ground true.\n"
      ],
      "metadata": {
        "id": "8mAtK98XdoNX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9okt1TtU2H0"
      },
      "outputs": [],
      "source": [
        "# gt_names=[]\n",
        "# for lable in gt_labels:\n",
        "#     gt_names.append(index2name[label])\n",
        "# show_boxes(image,torch.tensor(list(gt_boxes)),gt_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wbc8j4bgCDCt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPASExMKgvlj7f6Jyw0E7/r",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}